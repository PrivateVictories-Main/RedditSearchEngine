"""AI integration with Groq (primary) and Gemini (fallback) for intelligent query generation and synthesis."""
import json
import re
from typing import Optional
from datetime import datetime

from groq import Groq
import google.generativeai as genai

from config import get_settings
from models import (
    GeneratedQueries,
    GitHubResult,
    HuggingFaceResult,
    RedditResult,
)

settings = get_settings()


# Configure Gemini if available
if settings.gemini_api_key:
    genai.configure(api_key=settings.gemini_api_key)


def get_groq_client(api_key: Optional[str] = None) -> Optional[Groq]:
    """Get Groq client if API key is configured (user's key takes priority)."""
    if api_key:
        return Groq(api_key=api_key)
    if settings.groq_api_key:
        return Groq(api_key=settings.groq_api_key)
    return None


def get_gemini_model() -> Optional[any]:
    """Get Gemini model if API key is configured."""
    if settings.gemini_api_key:
        try:
            return genai.GenerativeModel('gemini-1.5-flash')
        except Exception as e:
            print(f"âš ï¸ Gemini initialization error: {e}")
    return None


async def generate_search_queries(user_query: str, api_key: Optional[str] = None, extracted_content: Optional[dict] = None) -> GeneratedQueries:
    """
    Use AI (Groq primary, Gemini fallback) to convert user's natural language 
    into optimized search queries for each platform.
    
    Args:
        user_query: The user's search query
        api_key: Optional user-provided API key (takes priority over settings)
        extracted_content: Optional dict of extracted content from URLs for context
    """
    current_year = datetime.now().year
    current_month = datetime.now().strftime("%B %Y")
    
    # Try Groq first (primary)
    client = get_groq_client(api_key)
    
    if client:
        try:
            prompt = _build_query_prompt(user_query, current_year, current_month, extracted_content)
            
            response = client.chat.completions.create(
                model="llama3-8b-8192",
                messages=[
                    {"role": "system", "content": "You are a search query optimization assistant. Always respond with valid JSON only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=300,
            )
            
            content = response.choices[0].message.content.strip()
            
            # Parse JSON from response
            json_match = re.search(r'\{[^{}]*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                return GeneratedQueries(
                    github_query=data.get("github_query", user_query),
                    huggingface_query=data.get("huggingface_query", user_query),
                    reddit_query=data.get("reddit_query", user_query),
                    reasoning=data.get("reasoning", "Generated by Groq AI"),
                )
        except Exception as e:
            print(f"âš ï¸ Groq query generation failed: {e}")
            print("ðŸ”„ Switching to Gemini fallback...")
    
    # Try Gemini fallback
    gemini_model = get_gemini_model()
    if gemini_model:
        try:
            prompt = _build_query_prompt(user_query, current_year, current_month, extracted_content)
            
            response = gemini_model.generate_content(prompt)
            content = response.text.strip()
            
            # Parse JSON from response
            json_match = re.search(r'\{[^{}]*\}', content, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
                print("âœ… Gemini fallback successful")
                return GeneratedQueries(
                    github_query=data.get("github_query", user_query),
                    huggingface_query=data.get("huggingface_query", user_query),
                    reddit_query=data.get("reddit_query", user_query),
                    reasoning=data.get("reasoning", "Generated by Gemini AI (fallback)"),
                )
        except Exception as e:
            print(f"âš ï¸ Gemini fallback also failed: {e}")
    
    # Ultimate fallback - rule-based
    return _generate_fallback_queries(user_query, current_year)


async def synthesize_results(
    user_query: str,
    github_results: list[GitHubResult],
    huggingface_results: list[HuggingFaceResult],
    reddit_results: list[RedditResult],
    api_key: Optional[str] = None,
    extracted_content: Optional[dict] = None,
) -> str:
    """
    Use AI (Groq primary, Gemini fallback) to synthesize a comprehensive verdict 
    from all search results and extracted content.
    
    Args:
        user_query: The user's search query
        github_results: GitHub search results
        huggingface_results: HuggingFace search results
        reddit_results: Reddit search results
        api_key: Optional user-provided API key (takes priority over settings)
        extracted_content: Optional dict of extracted content from URLs
    """
    # Try Groq first (primary)
    client = get_groq_client(api_key)
    
    if client:
        try:
            prompt = _build_synthesis_prompt(user_query, github_results, huggingface_results, reddit_results, extracted_content)
            
            response = client.chat.completions.create(
                model="llama3-70b-8192",
                messages=[
                    {"role": "system", "content": "You are a real-time technical research advisor. Synthesize findings to provide actionable insights. Emphasize how recent the information is."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=300,
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âš ï¸ Groq synthesis failed: {e}")
            print("ðŸ”„ Switching to Gemini fallback...")
    
    # Try Gemini fallback
    gemini_model = get_gemini_model()
    if gemini_model:
        try:
            prompt = _build_synthesis_prompt(user_query, github_results, huggingface_results, reddit_results, extracted_content)
            
            response = gemini_model.generate_content(prompt)
            print("âœ… Gemini fallback successful")
            return response.text.strip()
            
        except Exception as e:
            print(f"âš ï¸ Gemini fallback also failed: {e}")
    
    # Ultimate fallback
    return _generate_fallback_synthesis(github_results, huggingface_results, reddit_results)


def _build_query_prompt(user_query: str, current_year: int, current_month: str, extracted_content: Optional[dict]) -> str:
    """Build the prompt for query generation."""
    content_context = ""
    if extracted_content:
        content_context = f"\n\nREAL-TIME CONTEXT FROM WEB:\n"
        for url, text in list(extracted_content.items())[:2]:
            if text:
                content_context += f"- {text[:300]}...\n"
    
    return f"""You are a search optimization expert focused on finding THE MOST RECENT information. Today is {current_month}.

USER'S PROJECT IDEA: "{user_query}"{content_context}

Generate search queries optimized for:
1. **GitHub** - Focus on: exact technical terms, primary libraries/frameworks, programming language. MUST include "{current_year}" or "recent" or "latest" for fresh projects.
2. **Hugging Face** - Focus on: specific model types, task names (e.g., "text-generation", "image-classification"), architecture names. MUST include "{current_year}" or "latest".
3. **Reddit** - Focus on: recent discussions. MUST include "{current_year}" or "recent" for latest info, plus keywords like "best", "recommendation", "current".

CRITICAL RULES:
- ALWAYS include "{current_year}" OR "recent" OR "latest" in EVERY query for maximum freshness
- Use time filter parameters when possible (e.g., "time:week", "time:month")
- Prioritize ACTIVELY MAINTAINED and RECENTLY UPDATED projects
- Use exact technical terminology
- Keep each query under 60 characters

Respond ONLY in this exact JSON format:
{{
    "github_query": "your github optimized query with {current_year}/recent",
    "huggingface_query": "your huggingface optimized query with {current_year}/latest", 
    "reddit_query": "your reddit optimized query with {current_year}/recent",
    "reasoning": "brief explanation emphasizing recency focus"
}}"""


def _build_synthesis_prompt(
    user_query: str,
    github_results: list[GitHubResult],
    huggingface_results: list[HuggingFaceResult],
    reddit_results: list[RedditResult],
    extracted_content: Optional[dict]
) -> str:
    """Build the prompt for synthesis."""
    # Prepare context
    github_context = []
    for r in github_results[:5]:
        status_emoji = {
            "active": "ðŸŸ¢",
            "maintained": "ðŸŸ¡", 
            "stale": "ðŸŸ ",
            "abandoned": "ðŸ”´",
        }.get(r.status.value, "âšª")
        github_context.append(f"- {r.title} ({status_emoji} {r.status.value}): {r.description or 'No description'}")
    
    hf_context = []
    for r in huggingface_results[:5]:
        hf_context.append(f"- {r.title}: {r.description or 'No description'}")
    
    reddit_context = []
    for r in reddit_results[:5]:
        warning = "âš ï¸ COMMUNITY WARNING" if r.has_warning else ""
        comments_summary = ""
        if r.top_comments:
            comments_summary = f" | Top comment: '{r.top_comments[0].body[:100]}...'"
        reddit_context.append(f"- r/{r.subreddit}: {r.title} {warning}{comments_summary}")
    
    content_context = ""
    if extracted_content:
        content_context = f"\n\nREAL-TIME CONTENT EXTRACTED FROM TOP RESULTS:\n"
        for url, text in list(extracted_content.items())[:3]:
            if text:
                content_context += f"- {text[:400]}...\n"
    
    return f"""You are a technical research advisor helping a developer find existing solutions using REAL-TIME data.

USER'S PROJECT IDEA: "{user_query}"

GITHUB REPOSITORIES FOUND:
{chr(10).join(github_context) if github_context else "No repositories found."}

HUGGING FACE MODELS/SPACES FOUND:
{chr(10).join(hf_context) if hf_context else "No models found."}

REDDIT COMMUNITY DISCUSSIONS:
{chr(10).join(reddit_context) if reddit_context else "No discussions found."}{content_context}

Based on these REAL-TIME findings, provide a concise verdict (3-4 sentences max):
1. Is there a strong existing solution the user can build upon?
2. What's the recommended starting point?
3. Any important warnings or recent trends from the community?
4. How recent/fresh is this information?

Be direct and actionable. Mention specific dates or time frames when available. Start with the bottom line."""


def _generate_fallback_queries(user_query: str, current_year: int) -> GeneratedQueries:
    """Generate fallback queries when both AIs fail."""
    # Extract technical terms
    words = re.findall(r'\b\w+\b', user_query.lower())
    
    # Add common programming languages if mentioned
    langs = ['python', 'javascript', 'java', 'cpp', 'c++', 'rust', 'go', 'typescript']
    lang_keywords = [w for w in words if w in langs]
    
    # Always add current year for maximum recency
    github_query = f"{user_query} {current_year} {''.join(lang_keywords[:1])}".strip()
    huggingface_query = f"{user_query} {current_year} latest".strip()
    reddit_query = f"{user_query} {current_year} best recommendation recent".strip()
    
    return GeneratedQueries(
        github_query=github_query,
        huggingface_query=huggingface_query,
        reddit_query=reddit_query,
        reasoning=f"Using optimized fallback queries with {current_year} for maximum recency (both AI providers unavailable)",
    )


def _generate_fallback_synthesis(
    github_results: list[GitHubResult],
    huggingface_results: list[HuggingFaceResult],
    reddit_results: list[RedditResult],
) -> str:
    """Generate a basic synthesis when AI is unavailable."""
    parts = []
    
    if github_results:
        active = [r for r in github_results if r.status.value == "active"]
        if active:
            parts.append(f"Found {len(active)} active GitHub repositories")
        else:
            parts.append(f"Found {len(github_results)} GitHub repositories")
    
    if huggingface_results:
        parts.append(f"{len(huggingface_results)} Hugging Face models/spaces")
    
    if reddit_results:
        warnings = [r for r in reddit_results if r.has_warning]
        if warnings:
            parts.append(f"{len(reddit_results)} Reddit discussions ({len(warnings)} with community warnings)")
        else:
            parts.append(f"{len(reddit_results)} Reddit discussions")
    
    if not parts:
        return "No relevant results found. Try refining your search query with more specific technical terms."
    
    return f"Search complete: {', '.join(parts)}. Review the results below for potential starting points. (Note: AI synthesis unavailable - both Groq and Gemini providers are currently unavailable)"
